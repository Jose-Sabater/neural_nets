{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.kaggle.com/code/pestipeti/pytorch-starter-fasterrcnn-train/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset:\n",
    "\"https://www.kaggle.com/code/artgor/object-detection-with-pytorch-lightning\"\n",
    "\"https://www.kaggle.com/code/pestipeti/pytorch-starter-fasterrcnn-train/notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR_INPUT = '/kaggle/input/global-wheat-detection'\n",
    "TRAIN_DIR = f\"../data/wheat/train\"\n",
    "TEST_DIR = f\"../data/wheat/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wheat/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147788</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[64.0, 619.0, 84.0, 95.0]</td>\n",
       "      <td>arvalis_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147789</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[292.0, 549.0, 107.0, 82.0]</td>\n",
       "      <td>arvalis_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147790</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[134.0, 228.0, 141.0, 71.0]</td>\n",
       "      <td>arvalis_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147791</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[430.0, 13.0, 184.0, 79.0]</td>\n",
       "      <td>arvalis_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147792</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[875.0, 740.0, 94.0, 61.0]</td>\n",
       "      <td>arvalis_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  width  height                         bbox     source\n",
       "147788  5e0747034   1024    1024    [64.0, 619.0, 84.0, 95.0]  arvalis_2\n",
       "147789  5e0747034   1024    1024  [292.0, 549.0, 107.0, 82.0]  arvalis_2\n",
       "147790  5e0747034   1024    1024  [134.0, 228.0, 141.0, 71.0]  arvalis_2\n",
       "147791  5e0747034   1024    1024   [430.0, 13.0, 184.0, 79.0]  arvalis_2\n",
       "147792  5e0747034   1024    1024   [875.0, 740.0, 94.0, 61.0]  arvalis_2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_read(path: str) -> np.ndarray:\n",
    "    img = Image.open(path)\n",
    "    img = np.asarray(img)\n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validate Split\n",
    "20% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3373"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = df[\"image_id\"].unique()\n",
    "image_list.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698 training images and 675 validation images\n"
     ]
    }
   ],
   "source": [
    "valid_image_list = image_list[-round(3373 * 0.2) :]\n",
    "train_image_list = image_list[: -round(3373 * 0.2)]\n",
    "print(\n",
    "    f\"{len(train_image_list)} training images and {len(valid_image_list)} validation images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_paths = [\n",
    "#     os.path.join(TRAIN_DIR, f\"{filename}.jpg\") for filename in train_image_list\n",
    "# ]\n",
    "# valid_paths = [\n",
    "#     os.path.join(TRAIN_DIR, f\"{filename}.jpg\") for filename in valid_image_list\n",
    "# ]\n",
    "# train_images = [PIL_read(f) for f in train_paths]\n",
    "# valid_images = [PIL_read(f) for f in valid_paths]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes\n",
    "Lets make the format into 4 columns of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [834.0, 222.0, 56.0, 36.0]\n",
       "1         [226.0, 548.0, 130.0, 58.0]\n",
       "2         [377.0, 504.0, 74.0, 160.0]\n",
       "3         [834.0, 95.0, 109.0, 107.0]\n",
       "4         [26.0, 144.0, 124.0, 117.0]\n",
       "                     ...             \n",
       "147788      [64.0, 619.0, 84.0, 95.0]\n",
       "147789    [292.0, 549.0, 107.0, 82.0]\n",
       "147790    [134.0, 228.0, 141.0, 71.0]\n",
       "147791     [430.0, 13.0, 184.0, 79.0]\n",
       "147792     [875.0, 740.0, 94.0, 61.0]\n",
       "Name: bbox, Length: 147793, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"bbox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bbox\"] = (\n",
    "    df[\"bbox\"].str.replace(\"[\", \"\", regex=False).str.replace(\"]\", \"\", regex=False)\n",
    ")\n",
    "df[[\"x\", \"y\", \"w\", \"h\"]] = (\n",
    "    df[\"bbox\"].str.split(\",\", expand=True).apply(pd.to_numeric, errors=\"coerce\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"bbox\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source      x      y      w      h\n",
       "0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n",
       "1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n",
       "2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n",
       "3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n",
       "4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_image_nr = 1\n",
    "# image = train_images[selected_image_nr]\n",
    "# image_id = image_list[selected_image_nr]\n",
    "# boxes = df[df[\"image_id\"] == image_id][[\"x\", \"y\", \"w\", \"h\"]].astype(int).values\n",
    "# boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "# boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "# for box in boxes:\n",
    "#     cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (220, 0, 0), 3)\n",
    "\n",
    "# ax.set_axis_off()\n",
    "# ax.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear memory\n",
    "# del train_images\n",
    "# del valid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_df shape: (122577, 8);  Validation_df shape: (25216, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df = df[df[\"image_id\"].isin(train_image_list)]\n",
    "valid_df = df[df[\"image_id\"].isin(valid_image_list)]\n",
    "print(f\"Training_df shape: {train_df.shape};  Validation_df shape: {valid_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wheat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        self.image_ids = dataframe[\"image_id\"].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        # get all records of that image_id\n",
    "        records = self.df[self.df[\"image_id\"] == image_id]\n",
    "        image = PIL_read(f\"{self.image_dir}/{image_id}.jpg\").astype(np.float32)\n",
    "        # image = cv2.imread(f\"{self.image_dir}/{image_id}.jpg\", cv2.IMREAD_COLOR)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        boxes = records[[\"x\", \"y\", \"w\", \"h\"]].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # Set here your labels, only one class for this dataset\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"img_id\"] = torch.tensor([index])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        if self.transforms:\n",
    "            sample = {\"image\": image, \"bboxes\": target[\"boxes\"], \"labels\": labels}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample[\"image\"]\n",
    "\n",
    "            target[\"boxes\"] = torch.tensor(sample[\"bboxes\"])\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose(\n",
    "        [A.Flip(0.5), ToTensorV2(p=1.0)],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose(\n",
    "        [ToTensorV2(p=1.0)],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\General\\03_Projects\\neural_nets\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python\\General\\03_Projects\\neural_nets\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# # load a model; pre-trained on COCO\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "#     weights=\"FasterRCNN_ResNet50_FPN_Weights.DEFAULT\"\n",
    "# )\n",
    "# load a model; pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # 1 class (wheat) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collate_fn function is used by the DataLoader to process the input data in batches.\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "train_dataset = WheatDataset(train_df, TRAIN_DIR, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, TRAIN_DIR, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and valid set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LetÂ´s get some samples from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "# examples = enumerate(train_data_loader)\n",
    "# batch_idx, (images, targets, image_ids) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(image.to(device) for image in images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = targets[0][\"boxes\"].cpu().numpy().astype(np.int32)\n",
    "sample = images[0].permute(1, 2, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (220, 0, 0), 3)\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.7918315473514882\n",
      "Iteration #100 loss: 0.8570312772404838\n",
      "Iteration #150 loss: 0.8452534319788857\n",
      "Iteration #200 loss: 0.8038154500725624\n",
      "Iteration #250 loss: 0.967792148695238\n",
      "Iteration #300 loss: 0.8165943277686709\n",
      "Iteration #350 loss: 0.7341394441595273\n",
      "Iteration #400 loss: 0.9020944098467849\n",
      "Iteration #450 loss: 0.6313035340825157\n",
      "Iteration #500 loss: 0.8855056619852486\n",
      "Iteration #550 loss: 0.7835965218041538\n",
      "Iteration #600 loss: 0.9741899441734624\n",
      "Iteration #650 loss: 0.8618736936451081\n",
      "Epoch #0 loss: 0.9226590766989357\n",
      "Iteration #700 loss: 0.9478959960359298\n",
      "Iteration #750 loss: 0.6344556888968363\n",
      "Iteration #800 loss: 0.8844971659014966\n",
      "Iteration #850 loss: 0.7784184843601033\n",
      "Iteration #900 loss: 0.7863307221222076\n",
      "Iteration #950 loss: 0.71720978258874\n",
      "Iteration #1000 loss: 0.7681602481158698\n",
      "Iteration #1050 loss: 0.829146007560338\n",
      "Iteration #1100 loss: 0.6390534679483189\n",
      "Iteration #1150 loss: 0.7646782628606876\n",
      "Iteration #1200 loss: 0.9004667447788712\n",
      "Iteration #1250 loss: 0.7395086465938079\n",
      "Iteration #1300 loss: 0.7529740348865129\n",
      "Iteration #1350 loss: 0.8804117377939619\n",
      "Epoch #1 loss: 0.813938875778707\n"
     ]
    }
   ],
   "source": [
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(valid_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fasterrcnn_resnet50_fpn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48bc61fd71cdb43ded536cce11f82988caa9cc8f1cf4096168b5d4863e16d38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
